{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MyAPP, master=local[2]) created by __init__ at <ipython-input-1-c8576e7c5b10>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c8576e7c5b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PYSPARK_SUBMIT_ARGS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"--master local[2] pyspark-shell\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MyAPP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 336\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MyAPP, master=local[2]) created by __init__ at <ipython-input-1-c8576e7c5b10>:7 "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.add_packages(\"graphframes:graphframes:0.8.0-spark3.0-s_2.12\")\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local[2] pyspark-shell\"\n",
    "sc = pyspark.SparkContext(appName = \"MyAPP\")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "sc.addPyFile('/home/msbd5003/graphframes-0.8.0-spark3.0-s_2.12.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon3 = spark.read.csv('data/coupon3_feature.csv',header=True, inferSchema=True)\n",
    "merchant3 = spark.read.csv('data/merchant3_feature.csv',header=True, inferSchema=True)\n",
    "user3 = spark.read.csv('data/user3_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant3 = spark.read.csv('data/user_merchant3.csv',header=True, inferSchema=True)\n",
    "other_feature3 = spark.read.csv('data/other_feature3.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon1 = spark.read.csv('data/coupon1_feature.csv',header=True, inferSchema=True)\n",
    "merchant1 = spark.read.csv('data/merchant1_feature.csv',header=True, inferSchema=True)\n",
    "user1 = spark.read.csv('data/user1_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant1 = spark.read.csv('data/user_merchant1.csv',header=True, inferSchema=True)\n",
    "other_feature1 = spark.read.csv('data/other_feature1.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon2 = spark.read.csv('data/coupon2_feature.csv',header=True, inferSchema=True)\n",
    "merchant2 = spark.read.csv('data/merchant2_feature.csv',header=True, inferSchema=True)\n",
    "user2 = spark.read.csv('data/user2_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant2 = spark.read.csv('data/user_merchant2.csv',header=True, inferSchema=True)\n",
    "other_feature2 = spark.read.csv('data/other_feature2.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930723\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "dataset3 = coupon3.join(merchant3,'Merchant_id')\n",
    "dataset3 = dataset3.join(user3,'User_id')\n",
    "#print(\"2\")\n",
    "#print(dataset3.columns)\n",
    "dataset3 = dataset3.join(user_merchant3,['User_id','Merchant_id'])\n",
    "#print(\"3\")\n",
    "#print(dataset3.columns)\n",
    "dataset3 = dataset3.join(other_feature3,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "#print(\"4\")\n",
    "#print(dataset3.columns)\n",
    "print(dataset3.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset3 = dataset3.dropDuplicates()\n",
    "print(dataset3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_re = udf(lambda x: 0 if(x == 'null') else x)\n",
    "weekend = udf(lambda x: 1 if(x in (6,7)) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset3 = dataset3.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset3 = dataset3.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset3 = dataset3.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset3 = dataset3.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next session is to get weekdays dummies and concatenate dummies and original DF\n",
    "hard to concat two spark DF, if find solution, \n",
    "then no need to tranfer to Pandas DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Weekday1  Weekday2  Weekday3  Weekday4  Weekday5  Weekday6  Weekday7\n",
      "0            0         1         0         0         0         0         0\n",
      "1            0         0         0         0         0         1         0\n",
      "2            0         0         0         0         1         0         0\n",
      "3            0         0         0         0         1         0         0\n",
      "4            0         0         0         0         1         0         0\n",
      "5            0         0         0         0         0         0         1\n",
      "6            1         0         0         0         0         0         0\n",
      "7            0         0         1         0         0         0         0\n",
      "8            0         0         0         0         0         0         1\n",
      "9            0         0         0         0         0         0         1\n",
      "10           0         0         0         0         0         0         1\n",
      "11           0         0         0         0         1         0         0\n",
      "12           0         0         1         0         0         0         0\n",
      "13           0         0         0         0         0         0         1\n",
      "14           0         0         0         1         0         0         0\n",
      "15           0         0         0         0         1         0         0\n",
      "16           0         0         0         0         1         0         0\n",
      "17           1         0         0         0         0         0         0\n",
      "18           0         0         1         0         0         0         0\n",
      "19           0         0         0         0         1         0         0\n",
      "20           0         0         0         1         0         0         0\n",
      "21           0         0         1         0         0         0         0\n",
      "22           0         0         0         0         0         1         0\n",
      "23           0         0         1         0         0         0         0\n",
      "24           0         0         0         0         1         0         0\n",
      "25           0         0         1         0         0         0         0\n",
      "26           0         1         0         0         0         0         0\n",
      "27           0         1         0         0         0         0         0\n",
      "28           1         0         0         0         0         0         0\n",
      "29           0         0         0         0         0         0         1\n",
      "...        ...       ...       ...       ...       ...       ...       ...\n",
      "7470         0         0         0         0         1         0         0\n",
      "7471         0         0         0         1         0         0         0\n",
      "7472         0         0         0         0         0         1         0\n",
      "7473         0         0         1         0         0         0         0\n",
      "7474         0         1         0         0         0         0         0\n",
      "7475         0         0         0         0         0         1         0\n",
      "7476         0         0         0         1         0         0         0\n",
      "7477         0         1         0         0         0         0         0\n",
      "7478         0         0         0         0         0         0         1\n",
      "7479         0         0         0         0         0         0         1\n",
      "7480         0         0         0         0         0         1         0\n",
      "7481         0         0         0         0         0         1         0\n",
      "7482         0         0         1         0         0         0         0\n",
      "7483         1         0         0         0         0         0         0\n",
      "7484         0         0         1         0         0         0         0\n",
      "7485         0         0         0         1         0         0         0\n",
      "7486         1         0         0         0         0         0         0\n",
      "7487         0         0         0         1         0         0         0\n",
      "7488         0         1         0         0         0         0         0\n",
      "7489         0         0         0         0         0         1         0\n",
      "7490         0         0         0         1         0         0         0\n",
      "7491         0         0         0         0         0         0         1\n",
      "7492         1         0         0         0         0         0         0\n",
      "7493         0         0         0         0         1         0         0\n",
      "7494         0         0         0         0         1         0         0\n",
      "7495         0         0         0         0         0         1         0\n",
      "7496         0         0         0         0         1         0         0\n",
      "7497         1         0         0         0         0         0         0\n",
      "7498         0         1         0         0         0         0         0\n",
      "7499         0         0         0         0         0         1         0\n",
      "\n",
      "[7500 rows x 7 columns]\n",
      "DataFrame[User_id: bigint, Coupon_id: bigint, Date_received: bigint, Merchant_id: bigint, Discount_rate: string, Distance: string, Day_of_week: bigint, Day_of_mouth: bigint, Days_distance: bigint, Coupon_count: bigint, Total_sales: bigint, Sales_use_coupon: bigint, Total_coupon: bigint, Merchant_min_distance: bigint, Merchant_max_distance: bigint, Merchant_mean_distance: double, Merchant_median_distance: bigint, Merchant_coupon_transfer_rate: double, Coupon_rate: double, count_merchant: bigint, user_min_distance: bigint, user_max_distance: bigint, user_mean_distance: double, user_median_distance: bigint, buy_use_coupon: bigint, buy_total: bigint, coupon_received: bigint, avg_user_date_datereceived_gap: double, min_user_date_datereceived_gap: bigint, max_user_date_datereceived_gap: bigint, buy_use_coupon_rate: double, User_merchant_buy_total: string, User_merchant_received: string, User_merchant_buy_use_coupon: bigint, User_merchant_any: string, User_merchant_buy_common: bigint, User_merchant_coupon_transfer_rate: double, User_merchant_coupon_buy_rate: double, User_merchant_coupon_rate: double, User_merchant_coupon_common_rate: double, Month_Receive_same_coupon_count: bigint, Month_Receive_all_coupon_count: bigint, Month_same_coupon_lastone: bigint, Month_same_coupon_firstone: bigint, Day_Receive_all_coupon_count: bigint, Day_Receive_same_coupon_count: bigint, Label_merchant_coupon_count: bigint, Label_user_merchant_coupon_count: bigint, Coupon_count_later: bigint, Is_weekend: string, Weekday1: bigint, Weekday2: bigint, Weekday3: bigint, Weekday4: bigint, Weekday5: bigint, Weekday6: bigint, Weekday7: bigint]\n"
     ]
    }
   ],
   "source": [
    "#dataset3_pd = dataset3.toPandas()\n",
    "#weekday_dummies = pd.get_dummies(dataset3_pd.Day_of_week)\n",
    "#weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "#print(weekday_dummies)\n",
    "#dataset3_temp = spark.createDataFrame(pd.concat([dataset3_pd,weekday_dummies],axis=1))\n",
    "#print(dataset3_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Weekday1: int, Weekday6: int, Weekday3: int, Weekday5: int, Weekday4: int, Weekday7: int, Weekday2: int, User_id: int, Coupon_id: int, Date_received: int, Merchant_id: int, Discount_rate: string, Distance: string, Day_of_week: int, Day_of_mouth: int, Days_distance: int, Coupon_count: int, Total_sales: int, Sales_use_coupon: int, Total_coupon: int, Merchant_min_distance: int, Merchant_max_distance: int, Merchant_mean_distance: double, Merchant_median_distance: int, Merchant_coupon_transfer_rate: double, Coupon_rate: double, count_merchant: int, user_min_distance: int, user_max_distance: int, user_mean_distance: double, user_median_distance: int, buy_use_coupon: int, buy_total: int, coupon_received: int, avg_user_date_datereceived_gap: double, min_user_date_datereceived_gap: int, max_user_date_datereceived_gap: int, buy_use_coupon_rate: double, User_merchant_buy_total: string, User_merchant_received: string, User_merchant_buy_use_coupon: int, User_merchant_any: string, User_merchant_buy_common: int, User_merchant_coupon_transfer_rate: double, User_merchant_coupon_buy_rate: double, User_merchant_coupon_rate: double, User_merchant_coupon_common_rate: double, Month_Receive_same_coupon_count: int, Month_Receive_all_coupon_count: int, Month_same_coupon_lastone: int, Month_same_coupon_firstone: int, Day_Receive_all_coupon_count: int, Day_Receive_same_coupon_count: int, Label_merchant_coupon_count: int, Label_user_merchant_coupon_count: int, Coupon_count_later: int, Is_weekend: string]\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "|Weekday2|Weekday7|Weekday4|Weekday6|Weekday5|Weekday1|Weekday3|\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "|       1|       0|       0|       0|       0|       0|       0|\n",
      "|       0|       0|       0|       1|       0|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       1|       0|       0|       0|       0|       0|\n",
      "|       0|       0|       0|       0|       0|       1|       0|\n",
      "|       0|       0|       0|       0|       0|       0|       1|\n",
      "|       0|       1|       0|       0|       0|       0|       0|\n",
      "|       0|       1|       0|       0|       0|       0|       0|\n",
      "|       0|       1|       0|       0|       0|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       0|       0|       0|       0|       0|       1|\n",
      "|       0|       1|       0|       0|       0|       0|       0|\n",
      "|       0|       0|       1|       0|       0|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "|       0|       0|       0|       0|       0|       1|       0|\n",
      "|       0|       0|       0|       0|       0|       0|       1|\n",
      "|       0|       0|       0|       0|       1|       0|       0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weekday = dataset3.select('Day_of_week').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [when(col('Day_of_week') == i,1).otherwise(0)\\\n",
    "            .alias(('Weekday'+str(i))) for i in weekday]\n",
    "dataset3 = dataset3.select(exprs+dataset3.columns)\n",
    "#print(dataset3)\n",
    "dataset3.select('Weekday2','Weekday7','Weekday4','Weekday6','Weekday5','Weekday1','Weekday3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weekday1', 'Weekday6', 'Weekday3', 'Weekday5', 'Weekday4', 'Weekday7', 'Weekday2', 'User_id', 'Coupon_id', 'Date_received', 'Discount_rate', 'Distance', 'Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Label_merchant_coupon_count', 'Label_user_merchant_coupon_count', 'Coupon_count_later', 'Is_weekend']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count']\n",
    "dataset3 = dataset3.drop(*drop_list)\n",
    "dataset3 = dataset3.replace('null',str(np.nan))\n",
    "print(dataset3.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.repartition(1).write.csv('dataset3.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352809\n",
      "5594\n",
      "['User_id', 'Coupon_id', 'Date_received', 'Merchant_id', 'Discount_rate', 'Distance', 'Date', 'Day_of_week', 'Day_of_mouth', 'Days_distance', 'Coupon_count', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count']\n"
     ]
    }
   ],
   "source": [
    "dataset2 = coupon2.join(merchant2,'Merchant_id')\n",
    "dataset2 = dataset2.join(user2,'User_id')\n",
    "\n",
    "dataset2 = dataset2.join(user_merchant2,['User_id','Merchant_id'])\n",
    "\n",
    "dataset2 = dataset2.join(other_feature2,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "\n",
    "print(dataset2.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset2 = dataset2.dropDuplicates()\n",
    "print(dataset2.count())\n",
    "print(dataset2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset2 = dataset2.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset2 = dataset2.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset2 = dataset2.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset2 = dataset2.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = udf(lambda x: 'null' if x is None else x.split(':'))\n",
    "get_label = udf(lambda x: 0 if x[0]=='null' else (\\\n",
    "            1 if (date(int(float(x[0][0:4])), int(float(x[0][4:6])), int(float(x[0][6:8])))-date(int(float(x[1][0:4])),int(float(x[1][4:6])),int(float(x[1][6:8])))).days<=15\\\n",
    "              else -1))\n",
    "add_date = udf(lambda x,y: 'null' if ((x is None) or (y is None)) else str(x)+':'+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Date|\n",
      "+--------+\n",
      "|20160522|\n",
      "|20160610|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|20160530|\n",
      "|20160620|\n",
      "|20160519|\n",
      "|20160520|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+\n",
      "|Date_received|\n",
      "+-------------+\n",
      "|     20160521|\n",
      "|     20160521|\n",
      "|     20160516|\n",
      "|     20160610|\n",
      "|     20160525|\n",
      "|     20160530|\n",
      "|     20160516|\n",
      "|     20160530|\n",
      "|     20160519|\n",
      "|     20160516|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+\n",
      "|            Label|\n",
      "+-----------------+\n",
      "|20160522:20160521|\n",
      "|20160610:20160521|\n",
      "|    null:20160516|\n",
      "|    null:20160610|\n",
      "|    null:20160525|\n",
      "|    null:20160530|\n",
      "|20160530:20160516|\n",
      "|20160620:20160530|\n",
      "|20160519:20160519|\n",
      "|20160520:20160516|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|               Label|\n",
      "+--------------------+\n",
      "|[20160522, 20160521]|\n",
      "|[20160610, 20160521]|\n",
      "|    [null, 20160516]|\n",
      "|    [null, 20160610]|\n",
      "|    [null, 20160525]|\n",
      "|    [null, 20160530]|\n",
      "|[20160530, 20160516]|\n",
      "|[20160620, 20160530]|\n",
      "|[20160519, 20160519]|\n",
      "|[20160520, 20160516]|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+\n",
      "|Label|\n",
      "+-----+\n",
      "|    1|\n",
      "|   -1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    1|\n",
      "|   -1|\n",
      "|    1|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataset2_pd = dataset2.toPandas()\n",
    "#weekday_dummies = pd.get_dummies(dataset2_pd.Day_of_week)\n",
    "#weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "#dataset2_pd\n",
    "#dataset2 = spark.createDataFrame(pd.concat([dataset2_pd,weekday_dummies],axis=1))\n",
    "#dataset2.select('Date').show(10)\n",
    "#dataset2.select('Date_received').show(10)\n",
    "\n",
    "weekday = dataset2.select('Day_of_week').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [when(col('Day_of_week') == i,1).otherwise(0)\\\n",
    "            .alias(('Weekday'+str(i))) for i in weekday]\n",
    "dataset2 = dataset2.select(exprs+dataset3.columns)\n",
    "#print(dataset2)\n",
    "dataset2.select('Weekday2','Weekday7','Weekday4','Weekday6','Weekday5','Weekday1','Weekday3').show()\n",
    "\n",
    "\n",
    "###\n",
    "dataset2 = dataset2.withColumn('Label',add_date(col('Date'),col('Date_received')))\n",
    "dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#dataset2 = dataset2.withColumn('Label',col('Label')+col('Date_received').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#print(dataset2.Label.)\n",
    "#dataset2 = dataset2.withColumn('Label', col('Label').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "dataset2 = dataset2.withColumn('Label',split_col(col('Label'))) \n",
    "dataset2.select('Label').show(10)\n",
    "dataset2 = dataset2.withColumn('Label',get_label(col('Label')))\n",
    "#dataset2.printSchema()\n",
    "dataset2.select('Label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User_id', 'Date_received', 'Discount_rate', 'Distance', 'Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Is_weekend', 'Weekday1', 'Weekday2', 'Weekday3', 'Weekday4', 'Weekday5', 'Weekday6', 'Weekday7', 'Label']\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count','Date','Date_recived','Coupon_id','Coupon_count']\n",
    "dataset2 = dataset2.drop(*drop_list)                               \n",
    "dataset2 = dataset2.replace('null',str(np.nan))\n",
    "print(dataset2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could not convert string to float\n",
    "dataset2.repartition(1).write.csv('dataset2.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3118743\n",
      "1994\n"
     ]
    }
   ],
   "source": [
    "dataset1 = coupon1.join(merchant1,'Merchant_id')\n",
    "dataset1 = dataset1.join(user1,'User_id')\n",
    "\n",
    "dataset1 = dataset1.join(user_merchant1,['User_id','Merchant_id'])\n",
    "\n",
    "dataset1 = dataset1.join(other_feature1,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "\n",
    "print(dataset1.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset1 = dataset1.dropDuplicates()\n",
    "print(dataset1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset1 = dataset1.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset1 = dataset1.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset1 = dataset1.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset1 = dataset1.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset1_pd = dataset1.toPandas()\n",
    "#weekday_dummies = pd.get_dummies(dataset1_pd.Day_of_week)\n",
    "#weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "#dataset1_pd\n",
    "#dataset1 = spark.createDataFrame(pd.concat([dataset1_pd,weekday_dummies],axis=1))\n",
    "#dataset1.select('Date').show(10)\n",
    "#dataset1.select('Date_received').show(10)\n",
    "\n",
    "weekday = dataset1.select('Day_of_week').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "exprs = [when(col('Day_of_week') == i,1).otherwise(0)\\\n",
    "            .alias(('Weekday'+str(i))) for i in weekday]\n",
    "dataset1 = dataset1.select(exprs+dataset3.columns)\n",
    "#print(dataset2)\n",
    "dataset1.select('Weekday2','Weekday7','Weekday4','Weekday6','Weekday5','Weekday1','Weekday3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Date|\n",
      "+--------+\n",
      "|    null|\n",
      "|20160607|\n",
      "|20160420|\n",
      "|20160506|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+\n",
      "|Date_received|\n",
      "+-------------+\n",
      "|     20160504|\n",
      "|     20160514|\n",
      "|     20160420|\n",
      "|     20160429|\n",
      "|     20160414|\n",
      "|     20160422|\n",
      "|     20160425|\n",
      "|     20160425|\n",
      "|     20160425|\n",
      "|     20160429|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+\n",
      "|            Label|\n",
      "+-----------------+\n",
      "|    null:20160504|\n",
      "|20160607:20160514|\n",
      "|20160420:20160420|\n",
      "|20160506:20160429|\n",
      "|    null:20160414|\n",
      "|    null:20160422|\n",
      "|    null:20160425|\n",
      "|    null:20160425|\n",
      "|    null:20160425|\n",
      "|    null:20160429|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|               Label|\n",
      "+--------------------+\n",
      "|    [null, 20160504]|\n",
      "|[20160607, 20160514]|\n",
      "|[20160420, 20160420]|\n",
      "|[20160506, 20160429]|\n",
      "|    [null, 20160414]|\n",
      "|    [null, 20160422]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160429]|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+\n",
      "|Label|\n",
      "+-----+\n",
      "|    0|\n",
      "|   -1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###\n",
    "dataset1 = dataset1.withColumn('Label',add_date(col('Date'),col('Date_received')))\n",
    "dataset1.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#dataset2 = dataset2.withColumn('Label',col('Label')+col('Date_received').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#print(dataset2.Label.)\n",
    "#dataset2 = dataset2.withColumn('Label', col('Label').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "dataset1 = dataset1.withColumn('Label',split_col(col('Label'))) \n",
    "dataset1.select('Label').show(10)\n",
    "dataset1 = dataset1.withColumn('Label',get_label(col('Label')))\n",
    "#dataset2.printSchema()\n",
    "dataset1.select('Label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count','Date','Date_recived','Coupon_id','Coupon_count']\n",
    "dataset1 = dataset1.drop(*drop_list)                               \n",
    "dataset1 = dataset1.replace('null',str(np.nan))\n",
    "#print(dataset1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.repartition(1).write.csv('dataset1.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
