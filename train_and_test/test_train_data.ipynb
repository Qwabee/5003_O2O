{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msbd5003/spark/python/pyspark/context.py:220: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.add_packages(\"graphframes:graphframes:0.8.0-spark3.0-s_2.12\")\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local[2] pyspark-shell\"\n",
    "sc = pyspark.SparkContext(appName = \"MyAPP\")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "sc.addPyFile('/home/msbd5003/graphframes-0.8.0-spark3.0-s_2.12.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon3 = spark.read.csv('data/coupon3_feature.csv',header=True, inferSchema=True)\n",
    "merchant3 = spark.read.csv('data/merchant3_feature.csv',header=True, inferSchema=True)\n",
    "user3 = spark.read.csv('data/user3_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant3 = spark.read.csv('data/user_merchant3.csv',header=True, inferSchema=True)\n",
    "other_feature3 = spark.read.csv('data/other_feature3.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon1 = spark.read.csv('data/coupon1_feature.csv',header=True, inferSchema=True)\n",
    "merchant1 = spark.read.csv('data/merchant1_feature.csv',header=True, inferSchema=True)\n",
    "user1 = spark.read.csv('data/user1_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant1 = spark.read.csv('data/user_merchant1.csv',header=True, inferSchema=True)\n",
    "other_feature1 = spark.read.csv('data/other_feature1.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon2 = spark.read.csv('data/coupon2_feature.csv',header=True, inferSchema=True)\n",
    "merchant2 = spark.read.csv('data/merchant2_feature.csv',header=True, inferSchema=True)\n",
    "user2 = spark.read.csv('data/user2_feature.csv',header=True, inferSchema=True)\n",
    "user_merchant2 = spark.read.csv('data/user_merchant2.csv',header=True, inferSchema=True)\n",
    "other_feature2 = spark.read.csv('data/other_feature2.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930723\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "dataset3 = coupon3.join(merchant3,'Merchant_id')\n",
    "dataset3 = dataset3.join(user3,'User_id')\n",
    "#print(\"2\")\n",
    "#print(dataset3.columns)\n",
    "dataset3 = dataset3.join(user_merchant3,['User_id','Merchant_id'])\n",
    "#print(\"3\")\n",
    "#print(dataset3.columns)\n",
    "dataset3 = dataset3.join(other_feature3,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "#print(\"4\")\n",
    "#print(dataset3.columns)\n",
    "print(dataset3.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset3 = dataset3.dropDuplicates()\n",
    "print(dataset3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_re = udf(lambda x: 0 if(x == 'null') else x)\n",
    "weekend = udf(lambda x: 1 if(x in (6,7)) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset3 = dataset3.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset3 = dataset3.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset3 = dataset3.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset3 = dataset3.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next session is to get weekdays dummies and concatenate dummies and original DF\n",
    "hard to concat two spark DF, if find solution, \n",
    "then no need to tranfer to Pandas DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User_id', 'Coupon_id', 'Date_received', 'Discount_rate', 'Distance', 'Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Label_merchant_coupon_count', 'Label_user_merchant_coupon_count', 'Coupon_count_later', 'Is_weekend', 'Weekday1', 'Weekday2', 'Weekday3', 'Weekday4', 'Weekday5', 'Weekday6', 'Weekday7']\n"
     ]
    }
   ],
   "source": [
    "dataset3_pd = dataset3.toPandas()\n",
    "weekday_dummies = pd.get_dummies(dataset3_pd.Day_of_week)\n",
    "weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset3 = spark.createDataFrame(pd.concat([dataset3_pd,weekday_dummies],axis=1))\n",
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count']\n",
    "dataset3 = dataset3.drop(*drop_list)\n",
    "dataset3 = dataset3.replace('null',str(np.nan))\n",
    "print(dataset3.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.repartition(1).write.csv('dataset3.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352809\n",
      "5594\n",
      "['User_id', 'Coupon_id', 'Date_received', 'Merchant_id', 'Discount_rate', 'Distance', 'Date', 'Day_of_week', 'Day_of_mouth', 'Days_distance', 'Coupon_count', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count']\n"
     ]
    }
   ],
   "source": [
    "dataset2 = coupon2.join(merchant2,'Merchant_id')\n",
    "dataset2 = dataset2.join(user2,'User_id')\n",
    "\n",
    "dataset2 = dataset2.join(user_merchant2,['User_id','Merchant_id'])\n",
    "\n",
    "dataset2 = dataset2.join(other_feature2,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "\n",
    "print(dataset2.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset2 = dataset2.dropDuplicates()\n",
    "print(dataset2.count())\n",
    "print(dataset2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset2 = dataset2.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset2 = dataset2.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset2 = dataset2.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset2 = dataset2.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = udf(lambda x: 'null' if x is None else x.split(':'))\n",
    "get_label = udf(lambda x: 0 if x[0]=='null' else (\\\n",
    "            1 if (date(int(float(x[0][0:4])), int(float(x[0][4:6])), int(float(x[0][6:8])))-date(int(float(x[1][0:4])),int(float(x[1][4:6])),int(float(x[1][6:8])))).days<=15\\\n",
    "              else -1))\n",
    "add_date = udf(lambda x,y: 'null' if ((x is None) or (y is None)) else str(x)+':'+str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Date|\n",
      "+--------+\n",
      "|20160522|\n",
      "|20160610|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|20160530|\n",
      "|20160620|\n",
      "|20160519|\n",
      "|20160520|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+\n",
      "|Date_received|\n",
      "+-------------+\n",
      "|     20160521|\n",
      "|     20160521|\n",
      "|     20160516|\n",
      "|     20160610|\n",
      "|     20160525|\n",
      "|     20160530|\n",
      "|     20160516|\n",
      "|     20160530|\n",
      "|     20160519|\n",
      "|     20160516|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+\n",
      "|            Label|\n",
      "+-----------------+\n",
      "|20160522:20160521|\n",
      "|20160610:20160521|\n",
      "|    null:20160516|\n",
      "|    null:20160610|\n",
      "|    null:20160525|\n",
      "|    null:20160530|\n",
      "|20160530:20160516|\n",
      "|20160620:20160530|\n",
      "|20160519:20160519|\n",
      "|20160520:20160516|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|               Label|\n",
      "+--------------------+\n",
      "|[20160522, 20160521]|\n",
      "|[20160610, 20160521]|\n",
      "|    [null, 20160516]|\n",
      "|    [null, 20160610]|\n",
      "|    [null, 20160525]|\n",
      "|    [null, 20160530]|\n",
      "|[20160530, 20160516]|\n",
      "|[20160620, 20160530]|\n",
      "|[20160519, 20160519]|\n",
      "|[20160520, 20160516]|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+\n",
      "|Label|\n",
      "+-----+\n",
      "|    1|\n",
      "|   -1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    1|\n",
      "|   -1|\n",
      "|    1|\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2_pd = dataset2.toPandas()\n",
    "weekday_dummies = pd.get_dummies(dataset2_pd.Day_of_week)\n",
    "weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset2_pd\n",
    "dataset2 = spark.createDataFrame(pd.concat([dataset2_pd,weekday_dummies],axis=1))\n",
    "dataset2.select('Date').show(10)\n",
    "dataset2.select('Date_received').show(10)\n",
    "\n",
    "###\n",
    "dataset2 = dataset2.withColumn('Label',add_date(col('Date'),col('Date_received')))\n",
    "dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#dataset2 = dataset2.withColumn('Label',col('Label')+col('Date_received').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#print(dataset2.Label.)\n",
    "#dataset2 = dataset2.withColumn('Label', col('Label').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "dataset2 = dataset2.withColumn('Label',split_col(col('Label'))) \n",
    "dataset2.select('Label').show(10)\n",
    "dataset2 = dataset2.withColumn('Label',get_label(col('Label')))\n",
    "#dataset2.printSchema()\n",
    "dataset2.select('Label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['User_id', 'Date_received', 'Discount_rate', 'Distance', 'Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Is_weekend', 'Weekday1', 'Weekday2', 'Weekday3', 'Weekday4', 'Weekday5', 'Weekday6', 'Weekday7', 'Label']\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count','Date','Date_recived','Coupon_id','Coupon_count']\n",
    "dataset2 = dataset2.drop(*drop_list)                               \n",
    "dataset2 = dataset2.replace('null',str(np.nan))\n",
    "print(dataset2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could not convert string to float\n",
    "dataset2.repartition(1).write.csv('dataset2.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3118743\n",
      "1994\n"
     ]
    }
   ],
   "source": [
    "dataset1 = coupon1.join(merchant1,'Merchant_id')\n",
    "dataset1 = dataset1.join(user1,'User_id')\n",
    "\n",
    "dataset1 = dataset1.join(user_merchant1,['User_id','Merchant_id'])\n",
    "\n",
    "dataset1 = dataset1.join(other_feature1,['User_id','Coupon_id','Date_received','Merchant_id'])\n",
    "\n",
    "print(dataset1.count())\n",
    "# only drop the row that is completely the same\n",
    "dataset1 = dataset1.dropDuplicates()\n",
    "print(dataset1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset1 = dataset1.withColumn('User_merchant_buy_total', null_re(col('User_merchant_buy_total')))\n",
    "dataset1 = dataset1.withColumn('User_merchant_any', null_re(col('User_merchant_any')))\n",
    "dataset1 = dataset1.withColumn('User_merchant_received', null_re(col('User_merchant_received')))\n",
    "dataset1 = dataset1.withColumn('Is_weekend', weekend(col('Day_of_week')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Date|\n",
      "+--------+\n",
      "|    null|\n",
      "|20160607|\n",
      "|20160420|\n",
      "|20160506|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "|    null|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+\n",
      "|Date_received|\n",
      "+-------------+\n",
      "|     20160504|\n",
      "|     20160514|\n",
      "|     20160420|\n",
      "|     20160429|\n",
      "|     20160414|\n",
      "|     20160422|\n",
      "|     20160425|\n",
      "|     20160425|\n",
      "|     20160425|\n",
      "|     20160429|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+\n",
      "|            Label|\n",
      "+-----------------+\n",
      "|    null:20160504|\n",
      "|20160607:20160514|\n",
      "|20160420:20160420|\n",
      "|20160506:20160429|\n",
      "|    null:20160414|\n",
      "|    null:20160422|\n",
      "|    null:20160425|\n",
      "|    null:20160425|\n",
      "|    null:20160425|\n",
      "|    null:20160429|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|               Label|\n",
      "+--------------------+\n",
      "|    [null, 20160504]|\n",
      "|[20160607, 20160514]|\n",
      "|[20160420, 20160420]|\n",
      "|[20160506, 20160429]|\n",
      "|    [null, 20160414]|\n",
      "|    [null, 20160422]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160425]|\n",
      "|    [null, 20160429]|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+\n",
      "|Label|\n",
      "+-----+\n",
      "|    0|\n",
      "|   -1|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1_pd = dataset1.toPandas()\n",
    "weekday_dummies = pd.get_dummies(dataset1_pd.Day_of_week)\n",
    "weekday_dummies.columns = ['Weekday'+str(i+1) for i in range(weekday_dummies.shape[1])]\n",
    "dataset1_pd\n",
    "dataset1 = spark.createDataFrame(pd.concat([dataset1_pd,weekday_dummies],axis=1))\n",
    "dataset1.select('Date').show(10)\n",
    "dataset1.select('Date_received').show(10)\n",
    "\n",
    "###\n",
    "dataset1 = dataset1.withColumn('Label',add_date(col('Date'),col('Date_received')))\n",
    "dataset1.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#dataset2 = dataset2.withColumn('Label',col('Label')+col('Date_received').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "#dataset2.select('Label'),printSchema()\n",
    "#print(dataset2.Label.)\n",
    "#dataset2 = dataset2.withColumn('Label', col('Label').cast(StringType()))\n",
    "#dataset2.select('Label').show(10)\n",
    "dataset1 = dataset1.withColumn('Label',split_col(col('Label'))) \n",
    "dataset1.select('Label').show(10)\n",
    "dataset1 = dataset1.withColumn('Label',get_label(col('Label')))\n",
    "#dataset2.printSchema()\n",
    "dataset1.select('Label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Merchant_id', 'Day_of_week','Coupon_count','Date','Date_recived','Coupon_id','Coupon_count']\n",
    "dataset1 = dataset1.drop(*drop_list)                               \n",
    "dataset1 = dataset1.replace('null',str(np.nan))\n",
    "#print(dataset1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.repartition(1).write.csv('dataset1.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
