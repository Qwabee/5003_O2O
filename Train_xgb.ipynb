{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.add_packages(\"graphframes:graphframes:0.8.0-spark3.0-s_2.12\")\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-0.90.jar, xgboost4j-spark-0.90.jar pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']=\" --master local[2] pyspark-shell\"\n",
    "sc = pyspark.SparkContext(appName = \"MyAPP\")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "sc.addPyFile('/home/msbd5003/graphframes-0.8.0-spark3.0-s_2.12.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = spark.read.csv('data/dataset1.csv',header=True, inferSchema=True)\n",
    "dataset2 = spark.read.csv('data/dataset2.csv',header=True, inferSchema=True)\n",
    "#dataset3 is for competition\n",
    "#dataset3 = spark.read.csv('data/dataset3.csv',header=True, inferSchema=True)\n",
    "\n",
    "print(dataset1.count())\n",
    "print(dataset2.count())\n",
    "print(dataset1.columns)\n",
    "print(len(dataset1.columns))\n",
    "print(len(dataset2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_to_zero = udf(lambda x : 0 if x == -1 else x)\n",
    "\n",
    "#for regression, label only 1 or 0\n",
    "dataset1 = dataset1.withColumn('Label',minus_to_zero(col('Label'))) \n",
    "dataset2 = dataset2.withColumn('Label',minus_to_zero(col('Label'))) \n",
    "print(dataset1.count())\n",
    "print(dataset2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cal_discount_rate(s):\n",
    "    s =str(s)\n",
    "    s = s.split(':')\n",
    "    \n",
    "    if len(s)==1:\n",
    "        #print(s[0])\n",
    "        return float(s[0])\n",
    "    else:\n",
    "        return float(1.0-float(s[1])/float(s[0]))\n",
    "    \n",
    "calc_discount_rate = udf(lambda x : cal_discount_rate(x))\n",
    "\n",
    "check_not_float = udf(lambda x : x if type(x) == float else float(x))\n",
    "    \n",
    "dataset_train = dataset2.union(dataset1)\n",
    "#dataset_train = dataset_train.replace\n",
    "dataset_train = dataset_train.withColumn('Distance',col('Distance').cast(IntegerType()))\n",
    "dataset_train.select('Discount_rate').show()\n",
    "dataset_train = dataset_train.withColumn('Discount_rate',calc_discount_rate(col('Discount_rate')))\n",
    "dataset_train = dataset_train.withColumn('Discount_rate',check_not_float(col('Discount_rate')))\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "dataset_train = dataset_train.withColumn('Discount_rate',col('Discount_rate').cast(DoubleType()))\n",
    "dataset_train = dataset_train.select([col(c).cast(DoubleType()) for c in dataset_train.columns])\n",
    "\n",
    "dataset_train.select('Discount_rate').show()\n",
    "dataset_train.printSchema()\n",
    "print(dataset_train.count())\n",
    "print(len(dataset_train.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['User_id']\n",
    "dataset_train = dataset_train.drop(*drop_list)\n",
    "print(dataset_train.count())\n",
    "print(dataset_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['Date_received', 'Discount_rate', 'Distance', 'Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Is_weekend', 'Weekday1', 'Weekday2', 'Weekday3', 'Weekday4', 'Weekday5', 'Weekday6', 'Weekday7']\n",
    "col_list_not_rate_distance = ['Date_received','Day_of_mouth', 'Days_distance', 'Total_sales', 'Sales_use_coupon', 'Total_coupon', 'Merchant_min_distance', 'Merchant_max_distance', 'Merchant_mean_distance', 'Merchant_median_distance', 'Merchant_coupon_transfer_rate', 'Coupon_rate', 'count_merchant', 'user_min_distance', 'user_max_distance', 'user_mean_distance', 'user_median_distance', 'buy_use_coupon', 'buy_total', 'coupon_received', 'avg_user_date_datereceived_gap', 'min_user_date_datereceived_gap', 'max_user_date_datereceived_gap', 'buy_use_coupon_rate', 'user_coupon_transfer_rate', 'User_merchant_buy_total', 'User_merchant_received', 'User_merchant_buy_use_coupon', 'User_merchant_any', 'User_merchant_buy_common', 'User_merchant_coupon_transfer_rate', 'User_merchant_coupon_buy_rate', 'User_merchant_coupon_rate', 'User_merchant_coupon_common_rate', 'Label_user_merchant_coupon_count', 'Label_merchant_coupon_count', 'Coupon_count_later', 'Month_Receive_same_coupon_count', 'Month_Receive_all_coupon_count', 'Month_same_coupon_lastone', 'Month_same_coupon_firstone', 'Day_Receive_all_coupon_count', 'Day_Receive_same_coupon_count', 'Is_weekend', 'Weekday1', 'Weekday2', 'Weekday3', 'Weekday4', 'Weekday5', 'Weekday6', 'Weekday7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=col_list, outputCol = 'features', handleInvalid='skip')\n",
    "#assembler = VectorAssembler(inputCols=col_list, outputCol = 'features', handleInvalid='keep')\n",
    "#assembler = VectorAssembler(inputCols=col_list_not_rate, outputCol = 'features')\n",
    "#print(dataset_train.count())\n",
    "dataset_train = assembler.transform(dataset_train)\n",
    "dataset_train = dataset_train.drop(*col_list)\n",
    "print(dataset_train.count())\n",
    "dataset_train.printSchema()\n",
    "dataset_train.show()\n",
    "#dataset_train_ass.select('features').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train validation test 0.8:0.1:0.1  seed 1\n",
    "#rdd_dataset = dataset_train.randomSplit([0.8,0.1,0.1],1)\n",
    "rdd_dataset = dataset_train.randomSplit([0.9,0.1],1)\n",
    "#dataset3 is testing data set\n",
    "#feature_list = rdd_dataset[0].columns\n",
    "#feature_list.remove('Label')\n",
    "#rdd_dataset[0] = rdd_dataset[0].withColumn('features', concat_ws(\",\",array(*feature_list))).drop(*feature_list)\n",
    "#print(rdd_dataset[0].count())\n",
    "#rdd_dataset[0].show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "spark.sparkContext.addPyFile(\"sparkxgb.zip\") # read xgboost pyspark client lib\n",
    "from sparkxgb import XGBoostClassifier\n",
    "\n",
    "xgb = XGBoostClassifier(\n",
    "    objective=\"reg:logistic\",\n",
    "    maxDepth=5,\n",
    "    missing=float(0.0),\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Label\", \n",
    ")\n",
    "#rf = RandomForestRegressor(numTrees=15,maxDepth=6,labelCol = 'Label')\n",
    "#rf.explainParams()\n",
    "#xgb.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_dataset[0].printSchema()\n",
    "model1 = xgb.transform(rdd_dataset[0])\n",
    "#model1.getBootstrap()\n",
    "#model1.setLeafCol(\"leafID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = udf(lambda x : 1 if x>=0.5 else 0)\n",
    "\n",
    "prediction_v = model1.fit(rdd_dataset[1])\n",
    "\n",
    "prediction_v.printSchema()\n",
    "print(prediction_v.take(1))\n",
    "prediction_v = prediction_v.withColumn('prediction',threshold(col('prediction')))\n",
    "numSuccesses = prediction_v.where('Label == prediction').count()\n",
    "\n",
    "print(prediction_v.take(1))\n",
    "\n",
    "numInspections = prediction_v.count()\n",
    "\n",
    "print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))\n",
    "\n",
    "#prediction_t = model1.transform(rdd_dataset[2])\n",
    "#numSuccesses = prediction_t.where('Label == prediction').count()\n",
    "#numInspections = prediction_t.count()\n",
    "#print (\"There were %d inspections and there were %d successful predictions\" % (numInspections, numSuccesses))\n",
    "#print(\"This is a %d%% success rate\" % (float(numSuccesses) / float(numInspections) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
